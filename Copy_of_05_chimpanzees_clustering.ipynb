{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "625a2015",
      "metadata": {
        "id": "625a2015"
      },
      "source": [
        "# Practical 5: Clustering Chimpanzees\n",
        "\n",
        "### Austin Kaburia\n",
        "\n",
        "### Radzim Sendyka\n",
        "\n",
        "### 2025-09-22"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3be3625c",
      "metadata": {
        "id": "3be3625c"
      },
      "source": [
        "**Abstract**: In this self-guided practical we showcase a practical\n",
        "example of K-means clustering on Chimpanzee faces. Using a pre-trained\n",
        "classifier to generate a vector encoding of each portrait, we analyse\n",
        "the best parameter selection, and finally, implement K-means clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e17bf85",
      "metadata": {
        "id": "4e17bf85"
      },
      "source": [
        "$$\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b76469",
      "metadata": {
        "id": "34b76469"
      },
      "source": [
        "<!-- Do not edit this file locally. -->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!---->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!-- Do not edit this file locally. -->\n",
        "<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->\n",
        "<!--\n",
        "\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11ad1022",
      "metadata": {
        "id": "11ad1022"
      },
      "source": [
        "## Chimpanzee Faces\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_datasets/includes/chimpanzee-faces.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_datasets/includes/chimpanzee-faces.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "We know that human faces are unique to each of us. But did you know that\n",
        "chimpanzees also have unique faces?\n",
        "\n",
        "We will be checking if different images of the same chimpanzee would\n",
        "naturally cluster together. Using a sample set of 25 photos of 5\n",
        "chimpanzees from [Iashin et al.,\n",
        "2025](https://arxiv.org/abs/2507.10552), let’s see if we are able to\n",
        "cluster the dataset.\n",
        "\n",
        "Let’s download the photos (and other code we will need later). The\n",
        "[ChimpUFE](https://github.com/v-iashin/ChimpUFE) repo was created by\n",
        "Iashin et al. We will also be using their pre-trained neural networks\n",
        "for creating embeddings of the chimp’s faces.\n",
        "\n",
        "*Iashin, Vladimir, et al. “Self-supervised Learning on Camera Trap\n",
        "Footage Yields a Strong Universal Face Embedder.” arXiv preprint\n",
        "arXiv:2507.10552 (2025).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a48562a0",
      "metadata": {
        "id": "a48562a0"
      },
      "outputs": [],
      "source": [
        "#these files are quite large and take a couple minutes to download\n",
        "!git clone https://github.com/v-iashin/ChimpUFE.git\n",
        "!cd ChimpUFE && pip install -r requirements.txt\n",
        "!wget -P ./ChimpUFE/assets/weights https://github.com/v-iashin/ChimpUFE/releases/download/v1.0/25-06-06T20-51-36_330k.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ad6bfdd",
      "metadata": {
        "id": "7ad6bfdd"
      },
      "source": [
        "This is what the chimps look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8fb821",
      "metadata": {
        "id": "4c8fb821"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76f66010",
      "metadata": {
        "id": "76f66010"
      },
      "outputs": [],
      "source": [
        "gallery = \"./ChimpUFE/assets/gallery\"\n",
        "folders = sorted(os.listdir(gallery))\n",
        "\n",
        "fig, axes = plt.subplots(len(folders), 5, figsize=(10, 2*len(folders)))\n",
        "for i, f in enumerate(folders):\n",
        "    imgs = sorted(os.listdir(os.path.join(gallery, f)))[:5]\n",
        "    for j, img in enumerate(imgs):\n",
        "        ax = axes[i, j]\n",
        "        ax.imshow(mpimg.imread(os.path.join(gallery, f, img)))\n",
        "        ax.axis(\"off\")\n",
        "        if j == 0:\n",
        "            ax.text(-0.02, 0.5, f, transform=ax.transAxes, ha=\"right\", va=\"center\", fontsize=10)\n",
        "\n",
        "plt.subplots_adjust(left=0.18, wspace=0.3, hspace=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd5071f8",
      "metadata": {
        "id": "dd5071f8"
      },
      "source": [
        "## $k$ Means Clustering\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/kmeans-chimpanzee-faces.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/kmeans-chimpanzee-faces.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Let’s use the images directly to try to cluster the pictures. After\n",
        "extracting the pixel values, we will apply dimensionality reduction so\n",
        "that we can visualise the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5612ccc",
      "metadata": {
        "id": "d5612ccc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6af560cf",
      "metadata": {
        "id": "6af560cf"
      },
      "outputs": [],
      "source": [
        "paths = [os.path.join(gallery, f, img)\n",
        "         for f in sorted(os.listdir(gallery))\n",
        "         for img in sorted(os.listdir(os.path.join(gallery, f)))]\n",
        "\n",
        "images = np.array([mpimg.imread(p).ravel() for p in paths])\n",
        "print(images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd976c56",
      "metadata": {
        "id": "dd976c56"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b5ff26",
      "metadata": {
        "id": "17b5ff26"
      },
      "outputs": [],
      "source": [
        "images_2d = PCA(n_components=2).fit_transform(images)\n",
        "print(images_2d.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cada566f",
      "metadata": {
        "id": "cada566f"
      },
      "source": [
        "Let’s visualise our 2d mapping of the chimps. We can see that for the\n",
        "most part it’s not the worst, with many similar photos grouped together,\n",
        "but it’s surely not going to allow us to perfectly separate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa3b04b",
      "metadata": {
        "id": "baa3b04b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "import matplotlib.image as mpimg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bec6151",
      "metadata": {
        "id": "9bec6151"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "\n",
        "target_px = 32\n",
        "\n",
        "for (x, y), path in zip(images_2d, paths):\n",
        "    img = mpimg.imread(path)\n",
        "    h, w = img.shape[:2]\n",
        "    zoom = target_px / max(h, w)\n",
        "    ab = AnnotationBbox(OffsetImage(img, zoom=zoom), (x, y), frameon=False)\n",
        "    ax.add_artist(ab)\n",
        "\n",
        "mins = images_2d.min(axis=0); maxs = images_2d.max(axis=0)\n",
        "pad = 0.05 * (maxs - mins)\n",
        "ax.set_xlim(mins[0]-pad[0], maxs[0]+pad[0])\n",
        "ax.set_ylim(mins[1]-pad[1], maxs[1]+pad[1])\n",
        "ax.set_xticks([]); ax.set_yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e0aa4d",
      "metadata": {
        "id": "a6e0aa4d"
      },
      "source": [
        "## K-Means\n",
        "\n",
        "We will now apply K-means clustering on the mappings to group similar\n",
        "faces together.\n",
        "\n",
        "Let’s implement the algorithm as presented in the\n",
        "[lecture](https://mlatcl.github.io/mlfc/lectures/04-01-latent-variable-modelling.html).\n",
        "\n",
        "    1.  First, initialize cluster centres by randomly selecting k data points\n",
        "    2.  Assign each data point to its nearest cluster centre\n",
        "    3.  Update each cluster centre by computing the mean of all points assigned to it\n",
        "    4.  Repeat steps 2 and 3 until the cluster assignments stop changing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a495a386",
      "metadata": {
        "id": "a495a386"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Now, let’s implement K-means clustering. Let’s avoid using\n",
        "`scikit-learn`’s `KMeans`, or another imported library, as we want to\n",
        "implement it fram scratch, and visualise all the steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e265318",
      "metadata": {
        "id": "8e265318"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Exercise 1 here\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cluster_kmeans_handwritten(X, n_clusters=5, max_iter=30, random_state=24):\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    n_samples = X.shape[0]\n",
        "\n",
        "    # init centers by picking random points\n",
        "    centers = X[rng.choice(n_samples, n_clusters, replace=False)].copy()\n",
        "\n",
        "    history = []\n",
        "    for _ in range(max_iter):\n",
        "        dists = [[]] # TODO calculate distances\n",
        "\n",
        "        labels = [] # TODO assign to clusters\n",
        "\n",
        "        new_centers = [] # TODO update cluster centres\n",
        "\n",
        "        if True: # TODO stopping condition\n",
        "            break\n",
        "\n",
        "        history.append((labels.copy(), centers.copy()))\n",
        "        centers = new_centers\n",
        "    return history\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7b8045c",
      "metadata": {
        "id": "c7b8045c"
      },
      "source": [
        "Use the widget below to visualise the progress of your K-means\n",
        "clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38bda6ba",
      "metadata": {
        "id": "38bda6ba"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.image as mpimg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42eb0bbb",
      "metadata": {
        "id": "42eb0bbb"
      },
      "outputs": [],
      "source": [
        "def iterative_kmeans_widget(X, paths, n_clusters=5, max_iter=30, random_state=42):\n",
        "    history = cluster_kmeans_handwritten(X, n_clusters, max_iter, random_state)\n",
        "    X2 = PCA(n_components=2).fit_transform(X)\n",
        "\n",
        "    def show_iteration(counter=0):\n",
        "        labels, centers = history[counter]\n",
        "        centers2 = PCA(n_components=2).fit(X).transform(centers)\n",
        "\n",
        "        fig, (ax_scatter, ax_gallery) = plt.subplots(\n",
        "            1, 2, figsize=(14, 6),\n",
        "            gridspec_kw={'width_ratios':[2,3]}\n",
        "        )\n",
        "\n",
        "        cmap = plt.get_cmap(\"tab10\")\n",
        "        cluster_colors = {k: cmap(k) for k in range(n_clusters)}\n",
        "\n",
        "        # scatter plot with explicit color mapping\n",
        "        colors = [cluster_colors[l] for l in labels]\n",
        "        ax_scatter.scatter(X2[:,0], X2[:,1], c=colors, s=50)\n",
        "        ax_scatter.scatter(centers2[:,0], centers2[:,1], c=\"black\", marker=\"x\", s=100)\n",
        "        ax_scatter.set_title(f\"Iteration {counter}\")\n",
        "\n",
        "        # gallery\n",
        "        ax_gallery.axis(\"off\")\n",
        "        cols = len(X)//n_clusters*2\n",
        "        for k in range(n_clusters):\n",
        "            members = np.where(labels == k)[0][:cols]\n",
        "            y = n_clusters - k - 1\n",
        "            for j, idx in enumerate(members):\n",
        "                img = mpimg.imread(paths[idx])\n",
        "                ax_gallery.imshow(img, extent=[j, j+1, y, y+1])\n",
        "                ax_gallery.add_patch(\n",
        "                    plt.Rectangle((j, y), 1, 1, fill=False,\n",
        "                                  edgecolor=cluster_colors[k], lw=2)\n",
        "                )\n",
        "            ax_gallery.text(-0.5, y+0.5, f\"Cluster {k}\", va=\"center\", ha=\"right\")\n",
        "\n",
        "        ax_gallery.set_xlim(-1, cols)\n",
        "        ax_gallery.set_ylim(0, n_clusters)\n",
        "        ax_gallery.set_aspect(\"equal\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    slider = widgets.IntSlider(value=0, min=0, max=len(history)-1,\n",
        "                               step=1, description=\"Iteration\")\n",
        "    out = widgets.interactive_output(show_iteration, {\"counter\": slider})\n",
        "    display(slider, out)\n",
        "    return X2, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad24e257",
      "metadata": {
        "id": "ad24e257"
      },
      "outputs": [],
      "source": [
        "X2, history_i2d = iterative_kmeans_widget(images_2d, paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b44b6c66",
      "metadata": {
        "id": "b44b6c66"
      },
      "source": [
        "## Performance\n",
        "\n",
        "Reasoning about clustering performance is very diffucult, as there will\n",
        "often not be a simple cluster-class relationship, which will make it\n",
        "hard to say what image should be where. Many metrics exist, but none are\n",
        "perfect.\n",
        "\n",
        "Two approaches we will use here are: - Force a cluster-class\n",
        "correspondence by finding the permutation with the best accuracy. -\n",
        "Adjusted Rand Index (ARI) - counting pairs of images from the same class\n",
        "in the same cluster, adjusting for random chance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aa09f25",
      "metadata": {
        "id": "7aa09f25"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import adjusted_rand_score\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be41cdf1",
      "metadata": {
        "id": "be41cdf1"
      },
      "outputs": [],
      "source": [
        "def performance_stats(history, classes):\n",
        "  labels, _ = history[-1]\n",
        "  if len(classes) < 7:  # do you see why we can't run this for big inputs?\n",
        "    # Forcing a class-cluster correspondence\n",
        "    best = max(\n",
        "        ((np.mean([dict(zip(sorted(set(labels)), perm))[l]==c for l,c in zip(labels,classes)]), dict(zip(sorted(set(labels)), perm)))\n",
        "        for perm in itertools.permutations(sorted(set(classes)), len(set(labels)))),\n",
        "        key=lambda x: x[0]\n",
        "    )\n",
        "    # print('Mapping', best[1])\n",
        "    print('Best Accuracy:', best[0])\n",
        "  else:\n",
        "    print('Skipping Accuracy')\n",
        "\n",
        "  # ARI\n",
        "  ari = adjusted_rand_score(classes, labels)\n",
        "  print(\"ARI:\", round(ari,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f936eb6",
      "metadata": {
        "id": "7f936eb6"
      },
      "outputs": [],
      "source": [
        "classes = [x.split('/')[-2] for x in paths]\n",
        "performance_stats(history_i2d, classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4c49c1b",
      "metadata": {
        "id": "f4c49c1b"
      },
      "source": [
        "Both measures report that we did do better than random chance, but not\n",
        "by a lot. We can definetely improve on this, as pixel values taken alone\n",
        "do not carry sufficient information. To show this more intuitively - our\n",
        "approach didn’t even take into account which pixels were next to each\n",
        "other!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3318988",
      "metadata": {
        "id": "d3318988"
      },
      "source": [
        "## Face Embeddings\n",
        "\n",
        "This is where Neural Networks have a significant advantage, and we can\n",
        "use them to improve on the very naive clustering we did above. We will\n",
        "be using a pre-trained model provided by Iashin et al. These embeddings\n",
        "encode high-level facial features in a numerical vector space.\n",
        "\n",
        "We will now extract the embeddings for these 25 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb6eb4d3",
      "metadata": {
        "id": "cb6eb4d3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a49ade",
      "metadata": {
        "id": "e0a49ade"
      },
      "outputs": [],
      "source": [
        "def make_embeddings(path):\n",
        "  cwd = os.getcwd()\n",
        "  if os.path.basename(cwd) != \"ChimpUFE\":\n",
        "    os.chdir(\"ChimpUFE\") # needed to run code from demo_face_rec\n",
        "  from demo_face_rec import get_model, get_embedding\n",
        "\n",
        "  # setup\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model, tfm = get_model(\"./assets/weights/25-06-06T20-51-36_330k.pth\", device)\n",
        "  ds = datasets.ImageFolder(path, transform=tfm)\n",
        "  loader = DataLoader(ds, batch_size=8, num_workers=2, shuffle=False)\n",
        "\n",
        "  # extracting embeddings\n",
        "  embs = []\n",
        "  for x, _ in loader:\n",
        "      embs.append(get_embedding(model, x, device).cpu())\n",
        "  embeddings = torch.cat(embs, 0)\n",
        "  os.chdir(cwd)\n",
        "  return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b6a2e5",
      "metadata": {
        "id": "05b6a2e5"
      },
      "outputs": [],
      "source": [
        "embeddings = make_embeddings('./assets/gallery')\n",
        "print(embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e7ae9d1",
      "metadata": {
        "id": "6e7ae9d1"
      },
      "source": [
        "To be able to intuitively reason about the clustering and visualise it,\n",
        "we will again use dimensionality reduction to convert the encodings into\n",
        "2 dimensions. Note, that this will hurt performance, as we’re discarding\n",
        "most of the information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "616ee010",
      "metadata": {
        "id": "616ee010"
      },
      "outputs": [],
      "source": [
        "embeddings_2d = PCA(n_components=2).fit_transform(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9c19927",
      "metadata": {
        "id": "e9c19927"
      },
      "source": [
        "And now, we can reuse the code we wrote above to see if the clustering\n",
        "works better on the embeddings. It should!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "122d490c",
      "metadata": {
        "id": "122d490c"
      },
      "outputs": [],
      "source": [
        "X2, history_e2d = iterative_kmeans_widget(embeddings_2d, paths, random_state=2)\n",
        "# the reason i'm fiddling with random state is because quite often the clustering succeeds in 1 step, which is not very illustrative"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "910d98f3",
      "metadata": {
        "id": "910d98f3"
      },
      "source": [
        "And we again evaluate performance. The raw pixel value PCA achieved\n",
        "about `0.44` best accuracy and `0.13` ARI. Do we do better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4b6ea7",
      "metadata": {
        "id": "5e4b6ea7"
      },
      "outputs": [],
      "source": [
        "performance_stats(history_e2d, classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e0c4d30",
      "metadata": {
        "id": "5e0c4d30"
      },
      "source": [
        "Ee lost quite a lot of information by boiling everything down to 2\n",
        "dimensions. For completeness, let’s repeat the 2 above analyses without\n",
        "the PCA step - bear in mind that this will make the visualisations quite\n",
        "a bit useless, so we’ll skip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b87569e5",
      "metadata": {
        "id": "b87569e5"
      },
      "outputs": [],
      "source": [
        "print('Full Images')\n",
        "history_i = cluster_kmeans_handwritten(images, 5)\n",
        "performance_stats(history_i, classes)\n",
        "print('Full Embeddings')\n",
        "history_e = cluster_kmeans_handwritten(np.array(embeddings), 5)\n",
        "performance_stats(history_e, classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ce59e27",
      "metadata": {
        "id": "9ce59e27"
      },
      "source": [
        "Looks like there’s no big improvement, and actually, the PCA images\n",
        "beats full images! While the lack of improvement on 2d embeddings is\n",
        "unexpected (and probably with more data we would see a significant\n",
        "differences), the 2d PCA over raw images is actually expected to do\n",
        "better. Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9469e05",
      "metadata": {
        "id": "e9469e05"
      },
      "source": [
        "## Other datasets\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/kmeans-other-chimpanzees.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/kmeans-other-chimpanzees.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Another bigger dataset of labeled chimpanzee images was published here:\n",
        "[paper](https://pub.inf-cv.uni-jena.de/pdf/freytag2016chimpanzee.pdf),\n",
        "[github](https://github.com/cvjena/chimpanzee_faces?tab=readme-ov-file).\n",
        "\n",
        "*Alexander Freytag and Erik Rodner and Marcel Simon and Alexander Loos\n",
        "and Hjalmar Kühl and Joachim Denzler: “Chimpanzee Faces in the Wild:\n",
        "Log-Euclidean CNNs for Predicting Identities and Attributes of\n",
        "Primates,” German Conference on Pattern Recognition (GCPR), 2016 .*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a264c5c",
      "metadata": {
        "id": "6a264c5c"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cvjena/chimpanzee_faces.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71334159",
      "metadata": {
        "id": "71334159"
      },
      "source": [
        "The below text file contains a description of the dataset. We’re\n",
        "selecting only the Filename and Name, but you might use more data in\n",
        "other analyses. The below code converts it into a handy dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa81a14",
      "metadata": {
        "id": "5fa81a14"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2b0ad1a",
      "metadata": {
        "id": "f2b0ad1a"
      },
      "outputs": [],
      "source": [
        "ann = \"chimpanzee_faces/datasets_cropped_chimpanzee_faces/data_CTai/annotations_ctai.txt\" # you can also use data_CZoo\n",
        "with open(ann) as f:\n",
        "  recs = [{line.strip().split()[i]: line.strip().split()[i+1] for i in [0, 2]} for line in f]\n",
        "df_freytag = pd.DataFrame(recs)\n",
        "df_freytag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac6edffa",
      "metadata": {
        "id": "ac6edffa"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "*Assess* the data, and modify it as necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75c810a8",
      "metadata": {
        "id": "75c810a8"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Exercise 2 here\n",
        "\n",
        "\n",
        "# TODO assess the data and modify it as necessary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7383621",
      "metadata": {
        "id": "b7383621"
      },
      "source": [
        "Let’s select a subset of this, to use in our clusterings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "387c9c50",
      "metadata": {
        "id": "387c9c50"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c390e502",
      "metadata": {
        "id": "c390e502"
      },
      "outputs": [],
      "source": [
        "def sample_subset(df, n, min_, max_=None, seed=42):\n",
        "    if max_ is None:\n",
        "      max_ = min_\n",
        "    rng = random.Random(seed)\n",
        "    valid = [n for n, c in df[\"Name\"].value_counts().items() if c >= min_]\n",
        "    names = rng.sample(valid, n)\n",
        "    dfs = []\n",
        "    for name in names:\n",
        "        sub = df[df[\"Name\"] == name]\n",
        "        n = rng.randint(min_, min(max_, len(sub)))\n",
        "        dfs.append(sub.sample(n, random_state=seed))\n",
        "    out = pd.concat(dfs)\n",
        "    return out\n",
        "\n",
        "df_freytag_small = sample_subset(df_freytag, 10, 10) # 10 photos each for 10 chimps\n",
        "\n",
        "gallery_freytag = \"ChimpUFE/assets/gallery_freytag\"\n",
        "os.makedirs(gallery_freytag, exist_ok=True)\n",
        "base = \"chimpanzee_faces/datasets_cropped_chimpanzee_faces/data_CTai\"\n",
        "\n",
        "for _, row in df_freytag_small.iterrows():\n",
        "    identity = row[\"Name\"]\n",
        "    src = os.path.join(base, row[\"Filename\"])\n",
        "    os.makedirs(os.path.join(gallery_freytag, identity), exist_ok=True)\n",
        "    dst = os.path.join(gallery_freytag, identity, os.path.basename(row[\"Filename\"]))\n",
        "    shutil.copy(src, dst)\n",
        "\n",
        "paths_freytag = [os.path.join(gallery_freytag, f, img)\n",
        "         for f in sorted(os.listdir(gallery_freytag))\n",
        "         for img in sorted(os.listdir(os.path.join(gallery_freytag, f)))]\n",
        "\n",
        "df_freytag_small"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b580fa",
      "metadata": {
        "id": "a1b580fa"
      },
      "source": [
        "Finally, we can run the same embedding code on the new dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc8af49",
      "metadata": {
        "id": "6dc8af49"
      },
      "outputs": [],
      "source": [
        "embeddings_freytag = make_embeddings('./assets/gallery_freytag')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc7974a8",
      "metadata": {
        "id": "dc7974a8"
      },
      "outputs": [],
      "source": [
        "embeddings_freytag_2d = PCA(n_components=2).fit_transform(embeddings_freytag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c093246",
      "metadata": {
        "id": "8c093246"
      },
      "outputs": [],
      "source": [
        "X2, history_freytag_e2d = iterative_kmeans_widget(embeddings_freytag_2d, paths_freytag, n_clusters=10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94cb55de",
      "metadata": {
        "id": "94cb55de"
      },
      "outputs": [],
      "source": [
        "classes_freytag = df_freytag_small['Name'].values\n",
        "performance_stats(history_freytag_e2d, classes_freytag)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f42ff90d",
      "metadata": {
        "id": "f42ff90d"
      },
      "source": [
        "The adjusted Rand index tells us that our approach worked on the new\n",
        "dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ce712e1",
      "metadata": {
        "id": "1ce712e1"
      },
      "source": [
        "## Hierarchical Clustering\n",
        "\n",
        "<span class=\"editsection-bracket\" style=\"\">\\[</span><span\n",
        "class=\"editsection\"\n",
        "style=\"\"><a href=\"https://github.com/lawrennd/snippets/edit/main/_ml/includes/hierarchical-clustering-chimpanzees.md\" target=\"_blank\" onclick=\"ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/snippets/edit/main/_ml/includes/hierarchical-clustering-chimpanzees.md', 13);\">edit</a></span><span class=\"editsection-bracket\" style=\"\">\\]</span>\n",
        "\n",
        "Hierarchical clustering is an alternative method, where all elements are\n",
        "at first considered their own clusters, and repetetively join the\n",
        "closest clusters together, until only one remains. This is very nicely\n",
        "visualised using\n",
        "[dendrograms](https://en.wikipedia.org/wiki/Dendrogram)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d83dfd7c",
      "metadata": {
        "id": "d83dfd7c"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Using our earlier dataset, let’s conduct hierarchical clustering, and\n",
        "produce a dendrogram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03b1efc3",
      "metadata": {
        "id": "03b1efc3"
      },
      "outputs": [],
      "source": [
        "# Write your answer to Exercise 3 here\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cluster_hierarchical_handwritten(X):\n",
        "    n_samples = X.shape[0]\n",
        "    clusters = {i: [i] for i in range(n_samples)}\n",
        "    distances = np.full((2*n_samples-1, 2*n_samples-1), np.inf)\n",
        "\n",
        "    # compute pairwise distances\n",
        "    for i in range(n_samples):\n",
        "        for j in range(i+1, n_samples):\n",
        "            pass # TODO compute and set distances between samples\n",
        "\n",
        "    Z = []\n",
        "    next_cluster = n_samples\n",
        "\n",
        "    while len(clusters) > 1:\n",
        "        # find closest pair\n",
        "        keys = list(clusters.keys())\n",
        "        min_d, pair = np.inf, None\n",
        "        for i in range(len(keys)):\n",
        "            for j in range(i+1, len(keys)):\n",
        "                pass # TODO calculate distance between clusters, if new best, update min_d and pair\n",
        "\n",
        "        i, j = pair\n",
        "        new_cluster = clusters[i] + clusters[j]\n",
        "        Z.append([i, j, min_d, len(new_cluster)])\n",
        "        clusters[next_cluster] = new_cluster\n",
        "        del clusters[i], clusters[j]\n",
        "        next_cluster += 1\n",
        "\n",
        "    return np.array(Z)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a631bd",
      "metadata": {
        "id": "f3a631bd"
      },
      "outputs": [],
      "source": [
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "from scipy.cluster.hierarchy import dendrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92bb22f",
      "metadata": {
        "id": "e92bb22f"
      },
      "outputs": [],
      "source": [
        "def show_dendrogram(Z, paths, zoom=0.15):\n",
        "    fig, (ax_dendro, ax_imgs) = plt.subplots(2, 1, figsize=(13, 6), gridspec_kw={\"height_ratios\": [100, 1]})\n",
        "    dendro = dendrogram(Z, no_labels=True, ax=ax_dendro)\n",
        "    ax_dendro.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
        "    ax_imgs.set_xlim(ax_dendro.get_xlim())\n",
        "    ax_imgs.set_ylim(0, 1)\n",
        "    ax_imgs.axis(\"off\")\n",
        "    leaves = dendro[\"leaves\"]\n",
        "    xmin, xmax = ax_dendro.get_xlim()\n",
        "    margin = 0.02 * (xmax - xmin)\n",
        "    x_positions = np.linspace(xmin + margin, xmax - margin, num=len(leaves))\n",
        "\n",
        "    for x, leaf_idx in zip(x_positions, leaves):\n",
        "        img = mpimg.imread(paths[leaf_idx])\n",
        "        imagebox = OffsetImage(img, zoom=zoom)\n",
        "        ab = AnnotationBbox(imagebox, (x, 0.5), frameon=False)\n",
        "        ax_imgs.add_artist(ab)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9ca230f",
      "metadata": {
        "id": "a9ca230f"
      },
      "outputs": [],
      "source": [
        "linkage_output = cluster_hierarchical_handwritten(embeddings_2d)\n",
        "show_dendrogram(linkage_output, paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a8beaf9",
      "metadata": {
        "id": "5a8beaf9"
      },
      "source": [
        "End of Practical 5\n",
        "\n",
        "     _______  __   __  _______  __    _  ___   _  _______  __\n",
        "    |       ||  | |  ||   _   ||  |  | ||   | | ||       ||  |\n",
        "    |_     _||  |_|  ||  |_|  ||   |_| ||   |_| ||  _____||  |\n",
        "      |   |  |       ||       ||       ||      _|| |_____ |  |\n",
        "      |   |  |       ||       ||  _    ||     |_ |_____  ||__|\n",
        "      |   |  |   _   ||   _   || | |   ||    _  | _____| | __\n",
        "      |___|  |__| |__||__| |__||_|  |__||___| |_||_______||__|\n",
        "\n",
        "::: {.cell .markdown}\n",
        "\n",
        "## Thanks!\n",
        "\n",
        "For more information on these subjects and more you might want to check\n",
        "the following resources.\n",
        "\n",
        "-   company: [Trent AI](https://trent.ai)\n",
        "-   book: [The Atomic\n",
        "    Human](https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248)\n",
        "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
        "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
        "-   newspaper: [Guardian Profile\n",
        "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
        "-   blog:\n",
        "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}