{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bc3c1315",
      "metadata": {
        "id": "bc3c1315"
      },
      "source": [
        "# Practical 6: Machine Learning for Power Systems\n",
        "\n",
        "Instructors:\n",
        "- Oluwatomisin Dada, University of Cambridge\n",
        "- Cedric Kiplimo, Dedan Kimathi University of Technology\n",
        "\n",
        "15-09-2025\n",
        "\n",
        "**Abstract**: In this lab session, we will use a power system dataset to train two models (a linear model and a neural network) to solve an optimisation problem known as the alternating current optimal power flow (ACOPF), a rather difficult non-linear and non-convex optimisation problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "526a2c20",
      "metadata": {
        "id": "526a2c20"
      },
      "source": [
        "# Primer: Power Systems\n",
        "NOTE: You don't have to read this primer, although it is helpful to do so. If you wish to skip it, feel free to jump to [Section 7](#7-the-ieee-30-bus-test-case).\n",
        "\n",
        "Before getting into the machine learning aspects, we shall first get a basic introduction to power systems that will help us understand the dataset better.\n",
        "\n",
        "## 1. What is a Power System?\n",
        "A **power system** is a network that produces, transports, and consumes alternating-current (AC) electrical energy. Think of it as the complex system that comprises generation, transmission, distribution, and consumption of electrical power. It can be modelled as a graph where nodes are the _buses_ and edges are the _lines/branches_. Below is a sample graph representing a power system.\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://invenia.github.io/blog/public/images/power_grid_graphs.png\">\n",
        "</center>\n",
        "\n",
        "Let's define some terms.\n",
        "\n",
        "**Bus**  \n",
        "A _bus_ is simply a network node (a graph vertex) where equipment connects and where we measure or impose voltages and injections. Here are some examples:\n",
        "- Bus G: a generator terminal (power plant tied to the network).\n",
        "- Bus S: a substation feeding a neighborhood.\n",
        "- Bus L: a distribution node where many customer loads connect.\n",
        "\n",
        "At a bus we typically care about voltage (magnitude and *phase*) and how much power is being injected or withdrawn there.\n",
        "\n",
        "**Phase**  \n",
        "AC voltages and currents are sinusoids. A _phase_ refers to one such sinusoidal waveform. Many power systems use **three-phase** (three sinusoids separated by 120°)—that’s how large systems deliver power efficiently. When electrical engineers analyse power systems, they formulate equations known as power flow equations.\n",
        "\n",
        "For conceptual power-flow equations, they normally analyse one representative phase (single-phase equivalent) because steady-state three-phase, balanced systems reduce to three identical single-phase problems shifted in time. So when we talk about a voltage $V=V\\angle\\theta$  we refer to the magnitude and the _phase angle_ $\\theta$ (the time shift of the sinusoid).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2faa6385",
      "metadata": {
        "id": "2faa6385"
      },
      "source": [
        "## 2. Representing Voltages and Currents as Phasors\n",
        "A steady sinusoidal voltage is\n",
        "$$v(t)=V_m \\cos⁡(ωt+θ)$$\n",
        "\n",
        "where $V_m$​ is amplitude, $\\omega$ is angular frequency, and $\\theta$ is the phase offset.\n",
        "\n",
        "When everything is sinusoidal at the same $\\omega$ (steady-state), we replace $v(t)$ by its phasor\n",
        "$$\\mathbf{V} = V e^{j\\theta}$$\n",
        "where VVV commonly denotes RMS amplitude (engineer’s convention) and θ\\thetaθ is the phase angle. Similarly current $i(t)$ $\\leftrightarrow$ phasor $\\mathbf{I}=I e^{j\\phi}$.\n",
        "\n",
        "By doing this, differentiation/integration become algebraic multiplications by $j\\omega$, and Ohm’s/Kirchhoff’s laws become _algebraic equations in the complex domain_. This converts time-domain differential equations into linear algebra with complex numbers — huge simplification.\n",
        "\n",
        "## 3. AC Power is Complex\n",
        "A **port** is a terminal or pair of terminals through which a device exchanges electrical power with the network. Practically, a generator terminal is a port (the place it injects current/voltage to the grid).\n",
        "\n",
        "**Complex power** $S$ at a port is defined as $S=P+jQS$.\n",
        "    - $P$ (real/active power) is the energy per second doing useful work (watts).\n",
        "    - $Q$ (reactive power) is the power that oscillates back and forth because of inductors/capacitors — it doesn’t do long-term work but it affects voltages and currents.\n",
        "\n",
        "In phasor form we use:  \n",
        "  $$S=VI^*$$\n",
        "  where $V$ and $I$ are complex phasors and $I^*$ is the complex conjugate of the current phasor. This convention gives $P$ and $Q$ values that are independent of the arbitrary reference angle."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "945daa51",
      "metadata": {
        "id": "945daa51"
      },
      "source": [
        "## 4. Real, Reactive, and Apparent power\n",
        "Think of the grid as a big circuit built from resistors (R), inductors (L), and capacitors (C). These elements create different relationships between voltage and current:\n",
        "- **Resistor (R):** voltage and current are _in phase_. Energy is dissipated as heat → this is _real_ (active) power $P$.\n",
        "- **Inductor/Capacitor (L/C):** voltage and current are _out of phase_. Energy is alternately stored in fields and returned to the network → _reactive_ behavior $Q$. Over a cycle, ideal L/C do not consume net energy.\n",
        "\n",
        "**Formal quantities:**\n",
        "- **Complex power (apparent):** $S=P+jQS$.\n",
        "    - $P$ = real/active power (watts) = average power delivered/consumed.\n",
        "    - $Q$ = reactive power (vars) = oscillatory exchange due to L/C.\n",
        "    \n",
        "- **Apparent power magnitude:** $∣S∣= V_\\text{rms} I_\\text{rms}$.\n",
        "    \n",
        "- **Power factor:** $\\text{PF} = P / |S| = \\cos(\\phi)$, where $\\phi$ is the phase difference between voltage and current phasors.\n",
        "\n",
        "**Why reactive power matters operationally:**  \n",
        "Reactive power does not perform useful work but increases currents for the same real power. Higher currents lead to more $I^2 R$ losses, larger thermal stress on lines and transformers, and difficulty maintaining acceptable voltages. Operators must manage reactive power to keep voltages in safe ranges and limit losses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c20dc861",
      "metadata": {
        "id": "c20dc861"
      },
      "source": [
        "## 5. Power Flow Expressions\n",
        "### Part 1: The Simple Case\n",
        "Consider two buses $i$ and $j$ connected by a single series line. The quantity $p_{ij}$​ denotes the _real power flowing on that branch from bus $i$ toward bus $j$_ (positive means power leaves $i$ toward $j$). Similarly $q_{ij}$​ is the reactive power on that same directed branch.\n",
        "\n",
        "Let\n",
        "- $V_i = v_i e^{j\\theta_i}$ and $V_i = v_i e^{j\\theta_j}$ be the complex phasor voltages at the two buses (magnitudes $v$, angles $\\theta$).\n",
        "- The line series admittance is $y_{ij} = g_{ij} + j b_{ij}$ (the inverse of the line impedance $z_{ij}=r_{ij}+j x_{ij}$).\n",
        "- The angle difference $\\delta_{ij}:=\\theta_i - \\theta_j$\n",
        "\n",
        "Then, the current from $i$ to $j$  is\n",
        "$$I_{ij}=y_{ij}​(V_i​ − V_j​)$$\n",
        "The complex power at bus $i$ into the branch is\n",
        "$$\n",
        "\\begin{align}\n",
        "\tS_{ij} &= V_i I^*_{ij} \\\\\\\\\n",
        "\t&= V_i (y^*_{ij}​(V^*_i​ − V^*_j​)) = v^2_i y^*_{ij} - v_i v_j e^{j\\delta_{ij}}y^*_{ij}\n",
        "\\end{align}\n",
        "$$\n",
        "where $y^*_{ij} = g_{ij} - jb_{ij}$. We then take the real and imaginary parts to get the real and reactive branch powers, which are:\n",
        "* **Real power on branch $i$ → $j$** :\n",
        "  $$p_{ij} = v_i^2 g_{ij} - v_i v_j (g_{ij} \\cos \\delta_{ij} + b_{ij} \\sin \\delta_{ij})$$\n",
        "* **Reactive on branch $i$ → $j$**:\n",
        "  $$q_{ij} = -v_i^2 b_{ij} - v_i v_j (g_{ij} \\sin \\delta_{ij} - b_{ij} \\cos \\delta_{ij})$$\n",
        "\n",
        "Note the following:\n",
        "- The first term ($v_i^2 g_{ij}$ in $p_{ij}$ and $-v_i^2 b_{ij}$ in $q_{ij}$) are _local_ contributions (depend only on bus $i$ and the branch admittance).\n",
        "- The second term contains $v_i v_j$​ and trig of the angle difference: these are _coupling_ terms between buses $i$ and $j$.\n",
        "\n",
        "**Note the nonlinearity:** Terms like $v_i v_j \\cos\\delta_{ij}$ ​and $\\sin\\delta_{ij}$​ produce *products of magnitudes* and *trigonometric functions of angle differences*. Those are the nonlinear pieces that make AC power flow non-linear and non-convex.\n",
        "\n",
        "*However*, if you set $v_i = v_j =1\\;pu$ and assume small $\\delta_{ij}$​ and small $r$ vs $x$, you can linearize these expressions (that leads to the DC approximation).\n",
        "\n",
        "### Part 2: The General Case - Nodal Power Balance\n",
        "At a given bus $i$, many branches (lines) can meet. The **net complex power injected** at bus $i$ (positive if injected by generation, negative if consumed by loads) equals the sum of the complex powers on all branches leaving $i$ plus any shunt injections at the bus. This is the (complex) nodal power balance — it enforces Kirchhoff’s current law and defines the power-flow equations we solve.\n",
        "\n",
        "We end up with these equations:\n",
        "- **Nodal real power injection at bus $i$:**\n",
        "$$P_i \\;=\\; \\sum_{j=1}^N v_i v_j\\big(G_{ij}\\cos\\delta_{ij} + B_{ij}\\sin\\delta_{ij}\\big)$$\n",
        "- **Nodal reactive power injection at bus $j$:**\n",
        "$$Q_i \\;=\\; \\sum_{j=1}^N v_i v_j\\big(G_{ij}\\sin\\delta_{ij} - B_{ij}\\cos\\delta_{ij}\\big)$$\n",
        "\n",
        "### Part 3: What are we solving?\n",
        "We are given _specified_ injections at each bus (depending on bus type):\n",
        "\n",
        "- For a **PQ bus**: specified ($P_i^\\text{spec}$, $Q_i^\\text{spec}$).\n",
        "- For a **PV bus**: specified ($P_i^\\text{spec}$, $Q_i^\\text{spec}$) (reactive $Q_i$​ is unknown).\n",
        "- For the **slack bus**: specified ($V_i^\\text{spec}$, $\\theta_i^\\text{spec}$) (it absorbs any net mismatch).\n",
        "\n",
        "The **computed** injections from the current voltage guess are $P_i^\\text{calc}(V, \\theta)$, $Q_i^\\text{calc}(V, \\theta)$) given by the nodal formulas above. They can be obtained by simulation e.g. on MATLAB Simulink.\n",
        "\n",
        "The mismatch (power residuals) for each bus are:\n",
        "\n",
        "for each bus:\n",
        "$$\n",
        "\\begin{align}\n",
        "\t\\Delta P_i(V,\\theta) &\\;=\\; P_i^{\\text{spec}} \\;-\\; P_i^{\\text{calc}}(V,\\theta)\\\\[10pt]\n",
        "\t\\Delta Q_i(V,\\theta) &\\;=\\; Q_i^{\\text{spec}} \\;-\\; Q_i^{\\text{calc}}(V,\\theta)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Collect the relevant mismatches into a vector $F(x)$ where $x$ stacks the **unknown** voltage magnitudes and angles (the exact components of $x$ depend on bus types). For example, if all PQ buses have unknown $(v,\\theta)$ and PV buses have unknown $\\theta$, then\n",
        "\n",
        "$$\n",
        "F(x) = \\begin{bmatrix}\\text{(unknown angles)}\\\\[2pt]\\text{(unknown magnitudes)}\\end{bmatrix}, \\qquad F(x) = \\begin{bmatrix}\\Delta P\\\\[2pt]\\Delta Q\\end{bmatrix}\n",
        "$$\n",
        "(with rows corresponding only to the equations that must be satisfied — e.g., no $\\Delta Q$ row for a PV bus where $Q$ is not specified).\n",
        "\n",
        "Concisely put, our **goal** is to *find x (unknown voltages/angles) such that $F(x)=0$* i.e., _find the voltage magnitudes and angles so that every bus’s power balance holds_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c6caabb",
      "metadata": {
        "id": "6c6caabb"
      },
      "source": [
        "## 6. Optimal Power Flow (OPF)\n",
        "When we solve the power flow equations alone, we are simply finding voltages and currents consistent with physics. The only constraints we are following are physical ones. But in real power grids, we are concerned with more than just physics. For example, we might be interested in solving the power flow equations such that generator cost is minimised. This is especially useful in complex power grids where there many generators generating power at different costs. Our desire would be to *solve the power flow equations such that both the physical constraints and economic constraints are satisfied*.\n",
        "\n",
        "This is known as optimal power flow (OPF). The goal of OPF is to choose control variables (generator outputs, tap settings, etc.) to minimize an objective (cost, losses, whatever) subject to the physical power-flow equations and operational limits. To the power flow equations, we add:\n",
        "- Decision variables: generator real power $P_G$, reactive power $Q_G$, and voltage magnitudes $V$.\n",
        "- Objective function e.g., minimise cost\n",
        "- Constraints:\n",
        "\t- PF equations must hold\n",
        "\t- Generator limits ($P^{min}_G$, $P^{max}_G$ )\n",
        "\t- Voltage limits ($V^{min}_G$, $V^{max}_G$ )\n",
        "\t- Line limits (flows can't exceed ratings)\n",
        "\n",
        "In short, OPF says *choose generator setpoints and voltages that satisfy physics and minimize some objective*.\n",
        "\n",
        "### Part 1: AC-OPF\n",
        "Here we use the **full nonlinear PF equations** and solve it subject to chosen constraints.\n",
        "\n",
        "$$\n",
        "\\begin{aligned} \\min_{P_G, Q_G, V, \\theta} & \\quad \\sum_{g \\in \\text{generators}} C_g(P_{Gg}) \\\\ \\text{s.t.} & \\quad P_i = P_{Gi} - P_{Li}, \\\\ & \\quad Q_i = Q_{Gi} - Q_{Li}, \\\\ & \\quad \\text{(PF equations hold for all buses)} \\\\ & \\quad P_{G}^{\\min} \\le P_G \\le P_{G}^{\\max}, \\\\ & \\quad V^{\\min} \\le V \\le V^{\\max}, \\\\ & \\quad |S_{ij}| \\le S_{ij}^{\\max}. \\end{aligned}\n",
        "$$\n",
        "- $P_{Li}$, $Q_{Li}$: load demand at bus $i$\n",
        "- $C_g(\\cdot)$: generator cost (often quadratic)\n",
        "- $S_{ij}$: apparent power flow on line $i$-$j$\n",
        "\n",
        "AC-OPF is accurate but hard. The equalities are nonlinear and nonconvex (products $v_i v_j$​ and trig of angle differences). It is therefore a nonconvex constrained nonlinear program.\n",
        "\n",
        "### Part 2: DC-OPF\n",
        "To make OPF tractable, we simplify it as follows:\n",
        "- Assume all voltages are ~$1$ p.u. (ignore magnitude variation)\n",
        "- Ignore reactive power\n",
        "- Assume lines are mostly inductive (resistance $\\approx 0$)\n",
        "\n",
        "Then:\n",
        "$$P_{ij} = \\frac{1}{X_{ij}} (\\theta_i - \\theta_j)$$\n",
        "where $X_{ij}$ is line reactance.\n",
        "\n",
        "The nodal balance becomes:\n",
        "$$P_i = P_{Gi} - P_{Li} = \\sum_i \\frac{1}{X_{ij}} (\\theta_i - \\theta_j)$$\n",
        "which is linear system - much easier to solve.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\min_{P_G, \\theta} & \\quad \\sum_{g \\in \\text{generators}} C_g(P_{Gg}) \\\\ \\text{s.t.} & \\quad P_{Gi} - P_{Li} = \\sum_j \\tfrac{1}{X_{ij}}(\\theta_i - \\theta_j), \\\\ & \\quad P_G^{\\min} \\le P_G \\le P_G^{\\max}, \\\\ & \\quad |P_{ij}| \\le P_{ij}^{\\max}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "DCOPF is much faster to solve because it is linear (or convex quadratic). It is widely used in day-to-day market operations but is less accurate because it ignores voltage/reactive effects."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a09c55d",
      "metadata": {
        "id": "4a09c55d"
      },
      "source": [
        "## 7. The IEEE 30-bus Test Case\n",
        "It’s a **benchmark power system** model, originally published by the IEEE (Institute of Electrical and Electronics Engineers), that represents a *small but realistic transmission grid*. It is a simple approximation of a section of the American power grid as it was in 1961. Researchers and engineers use it to:\n",
        "- Test **power flow algorithms**.\n",
        "- Validate and compare **ACOPF/DCOPF solvers**.\n",
        "- Benchmark **optimization, machine learning, or control methods**.\n",
        "\n",
        "### Its Components\n",
        "This benchmark system comprises:\n",
        "- **30 buses (nodes)**: These are connection points — either loads, generators, or junctions.\n",
        "- **41 transmission lines/branches**: They connect buses, each with reactance, resistance, and line limits.\n",
        "- **6 generators**: Located at specific buses (e.g., bus 1 is the slack/reference bus). They supply real and reactive power within limits.\n",
        "- **Loads**: Many buses represent demand, specified in MW (real power) and Mvar (reactive power).\n",
        "- **Shunts**: Some buses include shunt elements (like capacitors) to support voltage.\n",
        "\n",
        "So it can be pictured as a **graph** with 30 nodes and 41 edges, but where each node has electrical variables (voltage, angle, load, generation).\n",
        "\n",
        "### Simulating the IEEE 30-bus Test Case on MATLAB\n",
        "This benchmark system can be simulated on MATLAB using MATPOWER, an open-source MATLAB/Octave package for **power system simulation and optimization**. Think of it as the **framework** (like PyTorch or TensorFlow) that loads the dataset and runs baseline solvers.\n",
        "\n",
        "It’s widely used by researchers and utilities for:\n",
        "- Solving **power flow (PF)** problems.\n",
        "- Running **optimal power flow (OPF)** (both AC and DC).\n",
        "- Benchmarking new algorithms.\n",
        "\n",
        "Think of it as a “toolbox” where you load a test case (like IEEE 30-bus) and run solvers. Using MATPOWER, you can run PF, OPF (AC and DC) to solve the power flow equations\n",
        "\n",
        "The dataset we will use in this practical is generated after running a simulation of the IEEE 30-bus test case in MATPOWER."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d24909e",
      "metadata": {
        "id": "7d24909e"
      },
      "source": [
        "## 8. Understanding and Using the Dataset\n",
        "In this lab, we shall use a dataset generated by solving AC-OPF of the IEEE 30-bus test case using MATPOWER. Our goal is to compare the performances of two machine learning models trained on this dataset at solving the AC-OPF problem. The simulation results were initially saved in the MATLAB `.mat` format but the code for extracting the data into familiar formats has been provided.\n",
        "\n",
        "Let's get into it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/kiplimock/ieee-case30-data.git\n",
        "! pip install osmnx"
      ],
      "metadata": {
        "id": "TJgGIblbZElV"
      },
      "id": "TJgGIblbZElV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "079f2abe",
      "metadata": {
        "id": "079f2abe"
      },
      "outputs": [],
      "source": [
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/ieee-case30-data/fynesse_mlfc\")\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c501292d",
      "metadata": {
        "id": "c501292d"
      },
      "outputs": [],
      "source": [
        "from fynesse.access import extract_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ef92351",
      "metadata": {
        "id": "8ef92351"
      },
      "source": [
        "First we need to load our data. At first, our data is stored in a dictionary with three keys - `train`, `val`, and `test`, which store the training, validation, and  test data we will use for our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd843ac",
      "metadata": {
        "id": "ddd843ac"
      },
      "outputs": [],
      "source": [
        "data = extract_data(data_path = \"/content/ieee-case30-data/matpower_format/case30\", standardise_outputs=False)\n",
        "keys = ['train', 'val', 'test']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "330867d5",
      "metadata": {
        "id": "330867d5"
      },
      "source": [
        "Each of these three dictionaries have four arrays that can be accesses using four keys - `X`, `Y`, `A`, `B`. These four arrays are the model inputs, model outputs, network adjacency matrices, and bus susceptances, respectively. For this lab, we shall mostly be interested in the model inputs and ouputs because we will train our models using them. We will also use the DCOPF data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56f6aa0",
      "metadata": {
        "id": "f56f6aa0"
      },
      "outputs": [],
      "source": [
        "data[keys[0]].keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd8718f",
      "metadata": {
        "id": "6cd8718f"
      },
      "source": [
        "### Examining the Data\n",
        "Before proceeding with any modelling, please take some time to understand the variables in the dataset by going through this [README](README.md) file. In particular, focus on the `X` and `Y` arrays. Try to gain some intutions for the meaning of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b16b301",
      "metadata": {
        "id": "9b16b301"
      },
      "source": [
        "#### Task 1\n",
        "Focusing on the train set, examine the inputs `X`. Notice the shape of the array.\n",
        "\n",
        "What do you think the various dimensions of the array correspond to?\n",
        "\n",
        "(**Hint:** Think back to the IEEE 30-bus test case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c37e28d5",
      "metadata": {
        "id": "c37e28d5"
      },
      "outputs": [],
      "source": [
        "X_train = data[keys[0]]['X']\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a176d9",
      "metadata": {
        "id": "d2a176d9"
      },
      "source": [
        "`X` train array has shape `(600, 30, 4)`.\n",
        "- Dimension 0 - the number of samples\n",
        "- Dimension 1 - the number of buses in the system\n",
        "- Dimension 2 - the number of features in the dataset i.e. $P_d$, $Q_d$, `mag_Sd` and `ang_Sd`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2071121",
      "metadata": {
        "id": "b2071121"
      },
      "source": [
        "#### Task 2\n",
        "Do the same for `Y`.\n",
        "\n",
        "**Question**: What do the dimensions correspond to?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b53520ed",
      "metadata": {
        "id": "b53520ed"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Y_train = ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73657639",
      "metadata": {
        "id": "73657639"
      },
      "source": [
        "#### Task 2 Extended\n",
        "Look at the test and validation sets as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e0e72e9",
      "metadata": {
        "id": "2e0e72e9"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f13b164",
      "metadata": {
        "id": "1f13b164"
      },
      "source": [
        "### Some Basic Prediction Models\n",
        "We will now use our data to see the performances of three basic solutions to the OPF problem:\n",
        "- **Gridwise averaging approach**: Computes an average for outputs across the whole grid, leading to a single output for the entire grid. This average can be used as a baseline predictor for voltage and phase at a bus.\n",
        "- **Nodewise averaging approach**: Computes an average for outputs per bus (node). So we begin with $n$ samples for each bus and aggregate those samples to end with $1$ for each bus. This average can be used as a baseline predictor for voltage and phase at a bus.\n",
        "- **DC-OPF**: The `dcopf` results from the simulation. It is a linearised simplification (and thus an estimate) of the ACOPF.\n",
        "\n",
        "Our comparison metrics will be mean absolute error (MAE), mean squared error (MSE), and fraction of variance unexplained (FVU)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b01de7d8",
      "metadata": {
        "id": "b01de7d8"
      },
      "source": [
        "### A. Gridwise Averaging\n",
        "\n",
        "To  average our outputs across the entire grid, we compute the mean of the array for each of the outputs (`Vm` and `Va`). For each output, the averaging yields a scalar result. Because we would like to use this result as a basline predictor, we will need to expand its dimensions to match its original shape for comparison.\n",
        "\n",
        "Notice how we are expanding our predictor output using `np.resize()`. Why did we do that? To understand why, check the shapes of `ym`, `np.resize(ym, data[key]['Y'][...,0].shape)`, and `data[key]['Y'][...,0]`. Do you notice that the shapes of `data[key]['Y'][...,0]` and `np.resize(ym, data[key]['Y'][...,0].shape)` are the same?\n",
        "\n",
        "We do the resizing because, for us to easily compare two arrays together using regression metrics like MSE, they should be of the same shape so that residuals can be computed using vectorised operations. This is very important. We will ensure we do this for all other approaches we study in this lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a3844d0",
      "metadata": {
        "id": "1a3844d0"
      },
      "outputs": [],
      "source": [
        "# Gridwise Averaging Approach --> average over entire grid\n",
        "ym, ya = data[keys[0]]['Y'][...,0].mean(), data[keys[0]]['Y'][...,1].mean()\n",
        "\n",
        "gridwise = {}\n",
        "for key in keys:\n",
        "    Vm_train = data[key]['Y'][...,0]\n",
        "    Va_train = data[key]['Y'][...,1]\n",
        "    gridwise.update({key : (np.resize(ym, Vm_train.shape), np.resize(ya, Va_train.shape))})\n",
        "\n",
        "# err_gw = print_err(target=data, pred_dict=gridwise)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f77f33",
      "metadata": {
        "id": "03f77f33"
      },
      "source": [
        "We saved our performance metrics in a dictionary called `gridwise`. The structure of this dictionary is important because we shall use it to store the metrics for all the \"models\" we shall use for prediction in this lab. This structure is as follows:\n",
        "```python\n",
        "{\n",
        "    \"train\": (Vm_train_pred, Va_train_pred),\n",
        "    \"val\": (Vm_val_pred, Va_val_pred),\n",
        "    \"test\": (Vm_test_pred, Va_test_pred)\n",
        "}\n",
        "```\n",
        "\n",
        "where `Vm_train_pred.shape = data[keys[0]]['Y'][...,0].shape` and `Va_train_pred.shape = data[keys[0]]['Y'][...,1].shape`. The same applies for `val` and `test`. Pay attention to this structure because you will need it when calculating the performance metrics in the next step.\n",
        "\n",
        "Let's now calculate the regression metrics for the train, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14cab35e",
      "metadata": {
        "id": "14cab35e"
      },
      "outputs": [],
      "source": [
        "err_dict = {}\n",
        "target = data.copy()\n",
        "for key in gridwise.keys():\n",
        "    complex_target = data[key]['Y'][:,:,0] * np.exp(1j * data[key]['Y'][:,:,1])\n",
        "    V_pred = gridwise[key]\n",
        "    V_target = target[key]['Y']\n",
        "    # print(key, V_pred[0].shape, V_target[:,:,0].shape)\n",
        "\n",
        "    if V_pred[0].shape == V_target[:,:,0].shape:\n",
        "        mag_err = V_pred[0] - V_target[:,:,0] # Vm\n",
        "        ang_err = V_pred[1] - V_target[:,:,1] # Va\n",
        "        complex_pred = gridwise[key][0] * np.exp(1j * gridwise[key][1])\n",
        "\n",
        "    else:\n",
        "        print(\"Shape mismatch! Broadcasting ...\")\n",
        "        mag_err = gridwise[key][0][0,:] - target[key]['Y'][:,:,0] # Vm\n",
        "        ang_err = gridwise[key][1][0:,] - target[key]['Y'][:,:,1] # Va\n",
        "        complex_pred = gridwise[key][0][0:,] * np.exp(1j * gridwise[key][1][0,:])\n",
        "\n",
        "    complex_err =  complex_pred - complex_target\n",
        "\n",
        "    print(f\"\\n{key.capitalize()} set metrics:\")\n",
        "    print(f\"Vm MAE: {np.abs(mag_err).mean():.5e}\")\n",
        "    print(f\"Vm MSE: {np.square(mag_err).mean():.5e}\")\n",
        "    print(f\"Vm FVU: {(np.square(mag_err).mean() / V_target[:,:,0].var()):.5e}\")\n",
        "\n",
        "    print(\"\")\n",
        "\n",
        "    print(f\"Va MAE: {np.abs(ang_err).mean():.5e}\")\n",
        "    print(f\"Va MSE: {np.square(ang_err).mean():.5e}\")\n",
        "    print(f\"Va FVU: {(np.square(ang_err).mean() / V_target[:,:,0].var()):.5e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09bef5db",
      "metadata": {
        "id": "09bef5db"
      },
      "source": [
        "#### Task 3 - Function to Calculate Metrics\n",
        "For reusability, create a function `print_err` to display the metrics and save them for later use. Most of the code for this has already been provided. You should add this function to you Access Assess Address pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820f7604",
      "metadata": {
        "id": "820f7604"
      },
      "outputs": [],
      "source": [
        "def print_err(target, pred_dict):\n",
        "    \"\"\"\n",
        "    Prints error metrics between target and predictions.\n",
        "    Args:\n",
        "        target (dict): Dictionary containing true values with keys 'train', 'val', 'test'.\n",
        "                       Each key maps to another dict with keys 'X', 'Y', 'A', and 'B'.\n",
        "        pred_dict (dict): Dictionary containing predicted values with keys 'train', 'val', 'test'.\n",
        "                          Each key maps to a tuple of (predicted_magnitudes, predicted_angles).\n",
        "\n",
        "    Returns:\n",
        "        err_dict (dict): Dictionary containing error metrics for each dataset split.\n",
        "    \"\"\"\n",
        "    err_dict = {} # for saving the metrics\n",
        "    for key in pred_dict.keys():\n",
        "        pass\n",
        "        # TODO\n",
        "\n",
        "        # 1. Calculate the error in predicted magnitudes and angles\n",
        "        # 2. Also calculate the error in complex form for completeness\n",
        "        # 3. Compute and print MAE, MSE, and FVU for each of these errors\n",
        "        # 4. Save and return these metrics in a dictionary for later use\n",
        "\n",
        "    # return err_dict\n",
        "\n",
        "    raise NotImplementedError(\"Function logic not yet implementd.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6681a350",
      "metadata": {
        "id": "6681a350"
      },
      "source": [
        "### B. Nodewise Averaging\n",
        "\n",
        "We average the outputs (`Vm` and `Va`) per bus/node. We begin with a `(n_samples x n_buses x n_outputs)` array for the entire system and end up with a `(n_buses x n_outputs)` array i.e. a vectors of size n_buses for each of the outputs.\n",
        "\n",
        "Let's try to think about how should perform this  averaging. We know that our arrays have shape `(n_samples, n_buses, n_outputs)`. This tells us that at each bus/node, we are measuring `n_output` variables `n_samples` times. Nodewise averaging means that for each of the variables, we average all the samples taken so that we end up with only one value at each node for each varible. For example, at bus/node 1, we have one value for var_1 and one for var_2. This is what we mean when we say we begin with a `(n_samples x n_buses x n_outputs)` array and end up with a `(n_buses x n_outputs)` array. Think about this can be implemented in NumPy.\n",
        "\n",
        "Afterwards, we will use the function we just created to compute the metrics of this baseline model.\n",
        "\n",
        "#### Task 4 - Implement Nodewise Averaging\n",
        "Now you have all you need to implement nodewise averaging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "278906a8",
      "metadata": {
        "id": "278906a8"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Nodewise Averaging Approach --> average per node\n",
        "# The code for computing ym and ya is very similar to the gridwise approach, but you need to make a small change. Think about what that change is.\n",
        "\n",
        "err_nw = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb490d6",
      "metadata": {
        "id": "7bb490d6"
      },
      "source": [
        "### C. DC-OPF\n",
        "\n",
        "We treat the DCOPF output of the simulation as a baseline model and compute the metrics. But first we need to access the DCOPF outputs from the simulation. We can do this by first accessing the array saved in the susceptance (`B`) key of `data[key]` dictionary e.g., `data['train]['B']` or `data['val']['B']`. This gives an array of length `n_samples` where each element is a dictionary with multiple keys, one of which is `V-dcopf` which we are interested in.\n",
        "\n",
        "So we can read one sample of DCOPF using `data['train']['B'][i]['V-dcopf']` for some `i`. Each sample, as you might guess is of shape `(n_buses, n_outputs)`, which in our case is (30, 2). You can see how this completes the picture because `n_samples` samples of shape `(n_buses, n_outputs)` makes an aggregate array of shape `(n_samples, n_buses, n_outputs)`. Now all we will need to do is write some clever code for accessing the DCOPF of each sample and then stack them together in a single array. Of course we will need to this for each set of data i.e., train, test, and validate.\n",
        "\n",
        "#### Task 5 - Check Performance of DC-OPF\n",
        "The code for preparing the data has been provided for you here so that you can focus on what you need to learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8eee9d6",
      "metadata": {
        "id": "d8eee9d6"
      },
      "outputs": [],
      "source": [
        "\n",
        "dcopf = {}\n",
        "for key in keys: # 'train', 'val', 'test'\n",
        "    tmp = np.array([item['V-dcopf'] for item in data[key]['B']]) # --> [n_samples, n_buses, n_outputs]\n",
        "    ym = tmp[..., 0]\n",
        "    ya = np.pi * (tmp[..., 1]/180) # convert degrees to radians\n",
        "\n",
        "    # TODO update the dcopf dictionary. Hint: look at how we did it for the gridwise and nodewise approaches.\n",
        "    dcopf.update()\n",
        "del tmp\n",
        "\n",
        "err_dc = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef6ebf1",
      "metadata": {
        "id": "0ef6ebf1"
      },
      "source": [
        "### D. Linear Model\n",
        "\n",
        "Now we turn our attention to a more sophisticated model - linear regression. Our goal is to build a model to predict `Vm` and `Va` at each bus using only two out of the four features - `Pd` and `Qd`. Our input is 3-dimensional with shape `(n_samples, n_buses, n_features)`. We can merge the last two dimensions so that the new shape is `(n_samples, n_buses * n_features)` so that our input has `n_buses * n_features` columns that we can use as features for our linear regression model.\n",
        "\n",
        "But there is a catch. Some buses are inactive i.e., there `Pd` and `Qd` values are zero. We should build our model with data from only active buses. So we'll drop readings from inactive buses. The new number of columns will be some $n \\leq n_{buses} \\times n_{features}$\n",
        "\n",
        "Something else. It's important to understand what the model we are building actually means in the context of our IEEE 30-bus power system. The outputs `Vm` and `Va` are measured at each bus. We want to predict these values at each bus too, and we are using the `Pd` and `Qd` measurements from all active buses in the grid to build our model. That means we will have one model to predict each output at each of the buses i.e., `n_buses` models to predict `Vm` and another `n_buses` to predict `Va`.\n",
        "\n",
        "#### Task 6 - Write a Function to Train Linear Regression Models\n",
        "Write the function `train_lm` to train and save these models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6885985d",
      "metadata": {
        "id": "6885985d"
      },
      "outputs": [],
      "source": [
        "inp = data[keys[0]]['X'][:,:,:2] # --> (n_samples, n_buses, n_features)\n",
        "out = data[keys[0]]['Y'] # --> (n_samples, n_buses, n_outputs)\n",
        "\n",
        "def train_lm(inp, out, model=LinearRegression):\n",
        "    \"\"\"\n",
        "    Trains linear regression models for each bus/node in the power system.\n",
        "\n",
        "    Args:\n",
        "        inp (np.ndarray): Input features of shape (n_samples, n_buses, n_features).\n",
        "        out (np.ndarray): Output targets of shape (n_samples, n_buses, n_outputs).\n",
        "        model (class): Linear regression model class from sklearn. Default is LinearRegression.\n",
        "\n",
        "    Returns:\n",
        "        mag_models (list): List of trained models for voltage magnitudes (Vm) at each bus.\n",
        "        ang_models (list): List of trained models for voltage angles (Va) at each bus\n",
        "    \"\"\"\n",
        "\n",
        "    model = LinearRegression\n",
        "\n",
        "    node_count = out.shape[1]\n",
        "    mag_models, ang_models = [], []\n",
        "\n",
        "    # TODO\n",
        "    # 1. Find index of nodes with nonzero loads and use as input to model\n",
        "    nonzero_indx = np.abs(inp[:,:,:2].mean(0)).sum(1) != 0 # boolean index of nonzero (active) load buses\n",
        "\n",
        "    # 2. Reshape the input to shape (n_samples, n_features*n_nonzero_buses). Use `nonzero_indx` as a mask to filter active buses\n",
        "    # 3. Train the models\n",
        "\n",
        "    # return mag_models, ang_models\n",
        "\n",
        "    raise NotImplementedError(\"Function logic not yet implementd.\")\n",
        "\n",
        "# train_lm(inp, out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9faa5ea6",
      "metadata": {
        "id": "9faa5ea6"
      },
      "source": [
        "We've written a helper function for you to obtain the predictions from the linear models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "012836ab",
      "metadata": {
        "id": "012836ab"
      },
      "outputs": [],
      "source": [
        "def get_predictions(models, inp):\n",
        "    shape = inp.shape[:2]\n",
        "    ym, ya = np.zeros(shape), np.zeros(shape)\n",
        "    nonzero_indx = np.abs(inp[:,:,:2].mean(0)).sum(1) != 0\n",
        "    inp_reshaped = inp[:,nonzero_indx,:2].reshape(-1, nonzero_indx.sum()*2) # --> (samples, n_features*n_nonzero_buses)\n",
        "\n",
        "    for i in range(shape[1]): # prediction for each bus yields array of shape [n_samples,]\n",
        "        ym[:,i] = models[0][i].predict(inp_reshaped)\n",
        "        ya[:,i] = models[1][i].predict(inp_reshaped)\n",
        "\n",
        "    return (ym, ya)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e159b58",
      "metadata": {
        "id": "1e159b58"
      },
      "source": [
        "#### Task 7 - Obtain Predictions of the Linear Model and Store Them\n",
        "Using the helper function provide, obtain the linear model's predictions and store them in a dictionary as you did with the other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11aba797",
      "metadata": {
        "id": "11aba797"
      },
      "outputs": [],
      "source": [
        "# Linear Model\n",
        "m_mod, a_mod = train_lm(data[keys[0]]['X'], data[keys[0]]['Y'], LinearRegression)\n",
        "lm = {}\n",
        "\n",
        "# TODO\n",
        "# store predictions in lm dictionary\n",
        "\n",
        "err_lm = print_err(target=data, pred_dict=lm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ff3cbba",
      "metadata": {
        "id": "1ff3cbba"
      },
      "source": [
        "The metrics of all our models have been saved in dictionaries. Now we need to visualise them. Try to think about the performance of the four models we have used so far - gridwise, averaging, nodewise averaging, DCOPF, and linear regression. Which do you think has the best performance and which one has the best performance? Think about how those models are built and make an educated guess about how they performed. Rank them in descending order of performance, say based on MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a37f2c9",
      "metadata": {
        "id": "5a37f2c9"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Rank the models based on MSE (your guess). Also discuss why you think they will perform the way you think?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7f58707",
      "metadata": {
        "id": "a7f58707"
      },
      "source": [
        "Now let's visualise the metrics. We begin with `MSE`, and then proceed to the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3045a6a3",
      "metadata": {
        "id": "3045a6a3"
      },
      "outputs": [],
      "source": [
        "errors = [err_gw, err_nw, err_dc, err_lm]\n",
        "err_key = \"MSE\"\n",
        "methods=['grid', 'node', 'dcopf', 'lm']\n",
        "\n",
        "show_mag, show_ang, log, sel_key = True, True, True, None\n",
        "rows = len(errors[0].keys()) if sel_key is None else len(sel_key)\n",
        "cols = int(show_mag) + int(show_ang)\n",
        "\n",
        "if err_key == \"MCEM\":\n",
        "    cols = 1\n",
        "    show_mag, show_ang = False, False\n",
        "if cols == 0:\n",
        "    raise ValueError(\"No columns in the figure\")\n",
        "\n",
        "fig, axs = plt.subplots(rows, cols, sharex=True)\n",
        "bar_colors = [\n",
        "    'tab:blue',\n",
        "    'tab:orange',\n",
        "    'tab:green',\n",
        "    'tab:red',\n",
        "    'tab:purple',\n",
        "    'tab:brown'\n",
        "    'tab:pink',\n",
        "    'tab:gray',\n",
        "    'tab:olive'\n",
        "    'tab:cyan',\n",
        "] * 5\n",
        "\n",
        "sel_key = list(errors[0].keys()) if sel_key is None else sel_key\n",
        "\n",
        "if rows == 1 and cols == 1:\n",
        "    sel_col = 'Vm' if show_mag else 'Va'\n",
        "    col_key = sel_col + ' ' + err_key\n",
        "    if err_key == \"MCEM\":\n",
        "        sel_col = 'Mean Complex Err Mag'\n",
        "        col_key = err_key\n",
        "    counts = [error[sel_key[0]][col_key] for error in errors]\n",
        "    if log:\n",
        "        axs.set_yscale('log')\n",
        "    axs.bar(methods, counts, color=bar_colors[:len(counts)])\n",
        "    axs.set_ylabel(err_key)\n",
        "    axs.set_title(sel_col)\n",
        "\n",
        "elif rows > 1:\n",
        "    for i, j in enumerate(sel_key):\n",
        "        if show_mag and show_ang:\n",
        "            counts = [error[j]['Vm ' + err_key] for error in errors]\n",
        "            if log:\n",
        "                axs[i, 0].set_yscale('log')\n",
        "                axs[i, 1].set_yscale('log')\n",
        "            axs[i, 0].bar(methods, counts, color=bar_colors[:len(counts)])\n",
        "            axs[i, 0].set_ylabel(err_key)\n",
        "            if i == 0:\n",
        "                axs[i, 0].set_title('Vm')\n",
        "\n",
        "            counts = [error[j]['Va ' + err_key] for error in errors]\n",
        "            axs[i, 1].bar(methods, counts, color=bar_colors[:len(counts)])\n",
        "            if i == 0:\n",
        "                axs[i, 1].set_title('Va')\n",
        "\n",
        "        else:\n",
        "            sel_col = 'Vm' if show_mag else 'Va'\n",
        "            col_key = sel_col+' '+err_key\n",
        "            if err_key == 'MCEM':\n",
        "                sel_col = 'Mean Complex Err Mag.'\n",
        "                col_key = err_key\n",
        "            counts = [error[sel_key[0]][col_key] for error in errors]\n",
        "            if log:\n",
        "                axs[i].set_yscale('log')\n",
        "            axs[i].bar(methods, counts, color=bar_colors[:len(counts)])\n",
        "            axs[i].set_ylabel(err_key)\n",
        "            if i == 0:\n",
        "                axs[i].set_title(sel_col)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03f13c4b",
      "metadata": {
        "id": "03f13c4b"
      },
      "source": [
        "For reusability, create a function `plot_comparison_bar` that we will use afterwards for plotting the comparison between the various models for a particular metric. The code for this function is already provided in the previous code cell, but you will need to make minor modifications to make it reusable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56659a08",
      "metadata": {
        "id": "56659a08"
      },
      "outputs": [],
      "source": [
        "def plot_comparison_bar(errors, err_key, methods, sel_key=None, *, show_mag=True, show_ang=True, logy=True):\n",
        "    \"\"\"\n",
        "    Plots a comparison bar chart for different error metrics across various methods.\n",
        "    \"\"\"\n",
        "    rows = len(errors[0].keys()) if sel_key is None else len(sel_key)\n",
        "    cols = int(show_mag)+int(show_ang)\n",
        "\n",
        "    # TODO\n",
        "    # Complete the function logic here. You can refer to the code above for guidance.\n",
        "\n",
        "    raise NotImplementedError(\"Function logic not yet implementd.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a643a9",
      "metadata": {
        "id": "b4a643a9"
      },
      "source": [
        "Using the function you just created, compete the four models you've used so far in terms of:\n",
        "1. FVU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c541c9b",
      "metadata": {
        "id": "6c541c9b"
      },
      "outputs": [],
      "source": [
        "plot_comparison_bar(errors=[err_gw, err_nw, err_dc, err_lm], err_key='FVU', methods=['grid', 'node', 'dcopf', 'lm'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47475584",
      "metadata": {
        "id": "47475584"
      },
      "source": [
        "2. MAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9d4a21",
      "metadata": {
        "id": "ad9d4a21"
      },
      "outputs": [],
      "source": [
        "plot_comparison_bar(errors=[err_gw, err_nw, err_dc, err_lm], err_key='MAE', methods=['grid', 'node', 'dcopf', 'lm'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8348676d",
      "metadata": {
        "id": "8348676d"
      },
      "source": [
        "3. MCEM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91d09e8e",
      "metadata": {
        "id": "91d09e8e"
      },
      "outputs": [],
      "source": [
        "plot_comparison_bar(errors=[err_gw, err_nw, err_dc, err_lm], err_key='MCEM', methods=['grid', 'node', 'dcopf', 'lm'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d97629e9",
      "metadata": {
        "id": "d97629e9"
      },
      "source": [
        "As you can see, for all the metrics, the worst performance was by the gridwise averging and DCOPF. This was expected because these models are too simple and do not capture the complexity of the grid. One averages across the entire grid while the other linearises a non-linear non-convex optimisation problem. Nodewise averaging was better than those two, but still not really good enough. Although averaging by bus captures more complexity, it's still a simple averaging operation. Linear regression is the best model by far. Not at all suprising because we might expect that there is a linear relationship between power (`Pd` and `Qd`) and voltages (`Vm` and `Va`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b26f2ee",
      "metadata": {
        "id": "7b26f2ee"
      },
      "source": [
        "### E. Neural Network\n",
        "Now we move on to something complex - neural networks. NNs often succeed where simpler models fail because of their ability to learn the non-linear relationships between various variables in a dataset. In this case, we shall train a simple fully-connected multi-layer perceptron (MLP).\n",
        "\n",
        "Try to think about how we should build the MLP.\n",
        "\n",
        "How many input and output \"neurons\" should we have?\n",
        "\n",
        "For the case of our dataset, do you expect the NN to do better than the linear model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca3eb74",
      "metadata": {
        "id": "aca3eb74"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# Size of input and output layers of the MLP\n",
        "# Performance of the MLP compared to the linear model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "199498ed",
      "metadata": {
        "id": "199498ed"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from fynesse.access import prep_loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a6922d7",
      "metadata": {
        "id": "4a6922d7"
      },
      "source": [
        "#### Task 8 - Define a class for an MLP\n",
        "Your task is to build an MLP with a hidden layer of 1024 units. The number of units in the input and output layers is determined by the nature of your data. If you answered the previous questions about the MLP correctly, you should be good to go here.\n",
        "\n",
        "The important thing to remember about building NNs in PyTorch is that your class should inherit from `nn.Module` and you must define two methods in your class - an `__init__` method where you define your players and a `forward` method where your forward pass operation is performed.\n",
        "\n",
        "Once your class definition is fine, consider adding it to your Access Assess Address framework. Where in the framework do you think it should go?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5b0af73",
      "metadata": {
        "id": "f5b0af73"
      },
      "outputs": [],
      "source": [
        "# MLP class definition\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        pass\n",
        "        # TODO\n",
        "        # Define the layers of the MLP here\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass\n",
        "        # TODO\n",
        "        # Define the forward pass\n",
        "\n",
        "        raise NotImplementedError(\"Function logic not yet implementd.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dde0dd0",
      "metadata": {
        "id": "8dde0dd0"
      },
      "source": [
        "In case you are new to PyTorch, we have defined a helper function `prep_loaders` for you to load your dataset and convert them into tensors so that we can take advantage of PyTorch's automatic differentiation utility and also train our model faster on a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa2b1c2",
      "metadata": {
        "id": "0aa2b1c2"
      },
      "outputs": [],
      "source": [
        "from fynesse.access import prep_loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f577958",
      "metadata": {
        "id": "3f577958"
      },
      "source": [
        "Now we need to prepare our data using this function to make them ready for passing into the NN. It is always good practice to scale your data before using them to train any model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c15a34aa",
      "metadata": {
        "id": "c15a34aa"
      },
      "outputs": [],
      "source": [
        "inp0, out0 = data[keys[0]]['X'], data[keys[0]]['Y']\n",
        "\n",
        "num_nodes = inp0.shape[1]\n",
        "num_samples = inp0.shape[0]\n",
        "\n",
        "# Normalize features\n",
        "scaler_inp = StandardScaler()\n",
        "scaler_out = StandardScaler()\n",
        "\n",
        "# Nonzero mask\n",
        "bool_nonzero_indx = (inp0[:,:,0] + inp0[:,:,1]).mean(0) != 0\n",
        "\n",
        "# Reshape from 3D to 2D --> [n_samples, n_buses*n_features]\n",
        "inp_shape = (num_samples, int(2*bool_nonzero_indx.sum()))\n",
        "\n",
        "# Select only 2 features (Pd & Qd) and nonzero load nodes\n",
        "sel_inp0 = inp0[:, bool_nonzero_indx, :2].reshape(inp_shape)\n",
        "sel_out0 = out0.reshape((num_samples, num_nodes*2))\n",
        "\n",
        "# scale the data\n",
        "scaler_inp.fit(sel_inp0)\n",
        "scaler_out.fit(sel_out0)\n",
        "\n",
        "# convert the data to tensors\n",
        "loaders = prep_loaders(data=data, keys=keys, scaler_inp=scaler_inp, scaler_out=scaler_out,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dc219b9",
      "metadata": {
        "id": "8dc219b9"
      },
      "source": [
        "Now it's time to train our NN using our scaled data. We will train our MLP in 200 epochs (feel free to experiment with fewer or more epochs). When training models in PyTorch, we need to provide the objective function (known as a criterion) and the algorithm for doing backward propagation (known as an optimiser). There are other quirks for training models with PyTorch, like disabling gradient calculation when running an inference on the model because we are not doing any training. There are many others, but there will suffice for now.\n",
        "\n",
        "We will track the training and validation losses while the NN is training so that we can visualise them later.It is possible to visualise the progress of training live by using [TensorBoard](https://docs.pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html), but this is outside the scope of this practical. For now, we shall just save the losses during the training and then visualise afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1f3c7a4",
      "metadata": {
        "id": "f1f3c7a4"
      },
      "outputs": [],
      "source": [
        "c_in, c_out = int(2*bool_nonzero_indx.sum()), int(2*num_nodes)\n",
        "\n",
        "# initialise model\n",
        "model = MLP(c_in=c_in, c_hidden=1024, c_out=c_out, num_layers=5, dp_rate=0.1)\n",
        "\n",
        "# specify loss fn and initialise optimiser\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "num_epochs = 200\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in tqdm.trange(num_epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "    for batch_x, batch_y in loaders[0]:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for val_x, val_y in loaders[1]:\n",
        "            val_outputs = model(val_x)\n",
        "            val_loss = criterion(val_outputs, val_y)\n",
        "            running_val_loss += val_loss.item()\n",
        "\n",
        "    avg_train_loss = running_train_loss / len(loaders[0])\n",
        "    avg_val_loss = running_val_loss / len(loaders[1])\n",
        "\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "running_test_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for test_x, test_y in loaders[2]:\n",
        "        test_outputs = model(test_x)\n",
        "        test_loss = criterion(test_outputs, test_y)\n",
        "        running_test_loss += test_loss.item()\n",
        "    avg_test_loss = running_test_loss / len(loaders[2])\n",
        "    print(f\"\\n Final Test MSE Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "# Plot loss curves\n",
        "# Loss curve shows some signs of overfitting the small dataset\n",
        "# likely contributes to this but can be mitigated using regularisation\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e5a2a28",
      "metadata": {
        "id": "8e5a2a28"
      },
      "source": [
        "We now need to run an inference on our trained model with the test data to obtain our predictions, and then proceed to obtain the metrics as we did with the other models for comparison. So we need to create a dictionary to store the metrics for the train, validation, and test data.\n",
        "\n",
        "#### Task 9 - Obtain the MLP Predictions of the Test Data and Get Performance Metrics\n",
        "Recall:\n",
        " - When running an inference on a trained model in PyTorch, what do you need to do?\n",
        "- You scaled your dataset before the training. What should do to the model predictions before computing the performance metrics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cae6df21",
      "metadata": {
        "id": "cae6df21"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# what do you need to do when running an inference on a trained model in PyTorch?\n",
        "# what should do to the model predictions before computing the performance metrics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0fc82de",
      "metadata": {
        "id": "b0fc82de"
      },
      "outputs": [],
      "source": [
        "loaders = prep_loaders(data=data, keys=keys, scaler_inp=scaler_inp, scaler_out=scaler_out, no_shuffle=True)\n",
        "\n",
        "mlp_preds = {}\n",
        "model.eval()\n",
        "\n",
        "for (key, loader) in zip(keys, loaders):\n",
        "    pass\n",
        "    # TODO\n",
        "    # 1. obtain model predictions\n",
        "    # 2. you scaled your dataset before the training. What should do to the model predictions before computing the performance metrics?\n",
        "    # 3. save the predictions in a dictionary for passing into the print_err function\n",
        "\n",
        "err_mlp = None # print_err(?)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8597c4c1",
      "metadata": {
        "id": "8597c4c1"
      },
      "source": [
        "Now plot a bar chart like the ones we've plotted before, but now including the NN's performance metrics. Compare the performance of all the models.\n",
        "\n",
        "- Comment on the performance of the MLP relative to that of DC-OPF and the linear model.\n",
        "- Is this performance what you expected? Why or why not?\n",
        "- What can you conclude from the results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c41dcb",
      "metadata": {
        "id": "97c41dcb"
      },
      "outputs": [],
      "source": [
        "plot_comparison_bar(errors=[err_gw, err_nw, err_dc, err_lm, err_mlp], err_key='MCEM', methods=['grid', 'node', 'dcopf', 'lm', 'mlp'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ad864c",
      "metadata": {
        "id": "34ad864c"
      },
      "source": [
        "#### End of Practical\n",
        "\n",
        "     _______  __   __  _______  __    _  ___   _  _______  __\n",
        "    |       ||  | |  ||   _   ||  |  | ||   | | ||       ||  |\n",
        "    |_     _||  |_|  ||  |_|  ||   |_| ||   |_| ||  _____||  |\n",
        "      |   |  |       ||       ||       ||      _|| |_____ |  |\n",
        "      |   |  |       ||       ||  _    ||     |_ |_____  ||__|\n",
        "      |   |  |   _   ||   _   || | |   ||    _  | _____| | __\n",
        "      |___|  |__| |__||__| |__||_|  |__||___| |_||_______||__|"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "594068e7",
      "metadata": {
        "id": "594068e7"
      },
      "source": [
        "## Thanks!\n",
        "\n",
        "For more information on these subjects and more you might want to check\n",
        "the following resources.\n",
        "\n",
        "-   company: [Trent AI](https://trent.ai)\n",
        "-   book: [The Atomic\n",
        "    Human](https://www.penguin.co.uk/books/455130/the-atomic-human-by-lawrence-neil-d/9780241625248)\n",
        "-   twitter: [@lawrennd](https://twitter.com/lawrennd)\n",
        "-   podcast: [The Talking Machines](http://thetalkingmachines.com)\n",
        "-   newspaper: [Guardian Profile\n",
        "    Page](http://www.theguardian.com/profile/neil-lawrence)\n",
        "-   blog:\n",
        "    [http://inverseprobability.com](http://inverseprobability.com/blog.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deep",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}